{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import ndarray as nd\n",
    "from hamiltonian.inference.sgd import sgd\n",
    "from hamiltonian.models.softmax import softmax\n",
    "\n",
    "model_ctx=mx.cpu()\n",
    "hyper={'alpha':1}\n",
    "in_units=X.shape[1]\n",
    "out_units=len(np.unique(y))\n",
    "model=softmax(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgd(model,model.par,step_size=0.01,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 89.79it/s]\n"
     ]
    }
   ],
   "source": [
    "par,loss=inference.fit(epochs=1000,batch_size=60,gamma=0.9,\n",
    "                       X_train=nd.array(X_train,ctx=model_ctx),y_train=nd.array(y_train,ctx=model_ctx),verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules re-loaded\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgd)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.weight': Parameter (shape=(3, 4), dtype=float32),\n",
       " '0.bias': Parameter (shape=(3,), dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(model.par,nd.array(X_test,ctx=model_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.78      0.78      0.78         9\n",
      "           2       0.78      0.78      0.78         9\n",
      "\n",
      "    accuracy                           0.87        30\n",
      "   macro avg       0.85      0.85      0.85        30\n",
      "weighted avg       0.87      0.87      0.87        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.int32(y_test),np.int32(y_pred.sample().asnumpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctx=mx.cpu()\n",
    "hyper={'alpha':25.}\n",
    "in_units=X.shape[1]\n",
    "out_units=len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.inference.hmc import hmc\n",
    "from hamiltonian.models.softmax import softmax\n",
    "\n",
    "model=softmax(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=hmc(model,model.par,step_size=0.1,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules re-loaded\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.hmc)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [05:16<00:00, 12.65it/s]\n",
      "100%|██████████| 4000/4000 [05:14<00:00, 12.71it/s]\n",
      "100%|██████████| 4000/4000 [05:22<00:00, 12.39it/s]\n",
      "100%|██████████| 4000/4000 [05:17<00:00, 12.58it/s]\n"
     ]
    }
   ],
   "source": [
    "samples=inference.sample(epochs=3000,burn_in=1000,path_length=1.0,chains=4,X_train=nd.array(X_train,ctx=model_ctx),y_train=nd.array(y_train,ctx=model_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "datasets=[az.convert_to_inference_data(sample) for sample in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = az.concat(datasets, dim=\"chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.weight[0,0]</th>\n",
       "      <td>-20.412</td>\n",
       "      <td>9.947</td>\n",
       "      <td>-34.531</td>\n",
       "      <td>-5.143</td>\n",
       "      <td>4.652</td>\n",
       "      <td>3.585</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[0,1]</th>\n",
       "      <td>-2.110</td>\n",
       "      <td>11.039</td>\n",
       "      <td>-17.662</td>\n",
       "      <td>18.704</td>\n",
       "      <td>5.407</td>\n",
       "      <td>4.128</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[0,2]</th>\n",
       "      <td>-15.377</td>\n",
       "      <td>7.504</td>\n",
       "      <td>-23.975</td>\n",
       "      <td>1.075</td>\n",
       "      <td>3.554</td>\n",
       "      <td>2.699</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[0,3]</th>\n",
       "      <td>13.160</td>\n",
       "      <td>18.892</td>\n",
       "      <td>-14.572</td>\n",
       "      <td>37.294</td>\n",
       "      <td>9.346</td>\n",
       "      <td>7.148</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[1,0]</th>\n",
       "      <td>11.575</td>\n",
       "      <td>6.443</td>\n",
       "      <td>3.800</td>\n",
       "      <td>24.498</td>\n",
       "      <td>3.128</td>\n",
       "      <td>2.385</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[1,1]</th>\n",
       "      <td>15.294</td>\n",
       "      <td>4.542</td>\n",
       "      <td>7.562</td>\n",
       "      <td>20.344</td>\n",
       "      <td>2.176</td>\n",
       "      <td>1.655</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[1,2]</th>\n",
       "      <td>-14.648</td>\n",
       "      <td>10.349</td>\n",
       "      <td>-29.156</td>\n",
       "      <td>0.175</td>\n",
       "      <td>5.029</td>\n",
       "      <td>3.835</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[1,3]</th>\n",
       "      <td>-15.303</td>\n",
       "      <td>11.559</td>\n",
       "      <td>-31.527</td>\n",
       "      <td>2.385</td>\n",
       "      <td>5.533</td>\n",
       "      <td>4.209</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[2,0]</th>\n",
       "      <td>-7.826</td>\n",
       "      <td>14.872</td>\n",
       "      <td>-35.445</td>\n",
       "      <td>6.964</td>\n",
       "      <td>7.284</td>\n",
       "      <td>5.561</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[2,1]</th>\n",
       "      <td>-0.700</td>\n",
       "      <td>7.884</td>\n",
       "      <td>-12.365</td>\n",
       "      <td>16.492</td>\n",
       "      <td>3.733</td>\n",
       "      <td>2.835</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[2,2]</th>\n",
       "      <td>-5.273</td>\n",
       "      <td>10.961</td>\n",
       "      <td>-23.688</td>\n",
       "      <td>9.338</td>\n",
       "      <td>5.330</td>\n",
       "      <td>4.064</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[2,3]</th>\n",
       "      <td>3.929</td>\n",
       "      <td>4.426</td>\n",
       "      <td>-4.822</td>\n",
       "      <td>10.049</td>\n",
       "      <td>2.055</td>\n",
       "      <td>1.555</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.bias[0]</th>\n",
       "      <td>0.244</td>\n",
       "      <td>6.192</td>\n",
       "      <td>-6.902</td>\n",
       "      <td>9.492</td>\n",
       "      <td>3.010</td>\n",
       "      <td>2.295</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.bias[1]</th>\n",
       "      <td>21.112</td>\n",
       "      <td>4.068</td>\n",
       "      <td>15.772</td>\n",
       "      <td>29.534</td>\n",
       "      <td>1.841</td>\n",
       "      <td>1.388</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.bias[2]</th>\n",
       "      <td>-8.665</td>\n",
       "      <td>7.806</td>\n",
       "      <td>-21.178</td>\n",
       "      <td>3.655</td>\n",
       "      <td>3.500</td>\n",
       "      <td>2.637</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean      sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "0.weight[0,0] -20.412   9.947 -34.531   -5.143      4.652    3.585       5.0   \n",
       "0.weight[0,1]  -2.110  11.039 -17.662   18.704      5.407    4.128       5.0   \n",
       "0.weight[0,2] -15.377   7.504 -23.975    1.075      3.554    2.699       5.0   \n",
       "0.weight[0,3]  13.160  18.892 -14.572   37.294      9.346    7.148       5.0   \n",
       "0.weight[1,0]  11.575   6.443   3.800   24.498      3.128    2.385       5.0   \n",
       "0.weight[1,1]  15.294   4.542   7.562   20.344      2.176    1.655       6.0   \n",
       "0.weight[1,2] -14.648  10.349 -29.156    0.175      5.029    3.835       5.0   \n",
       "0.weight[1,3] -15.303  11.559 -31.527    2.385      5.533    4.209       5.0   \n",
       "0.weight[2,0]  -7.826  14.872 -35.445    6.964      7.284    5.561       5.0   \n",
       "0.weight[2,1]  -0.700   7.884 -12.365   16.492      3.733    2.835       5.0   \n",
       "0.weight[2,2]  -5.273  10.961 -23.688    9.338      5.330    4.064       4.0   \n",
       "0.weight[2,3]   3.929   4.426  -4.822   10.049      2.055    1.555       5.0   \n",
       "0.bias[0]       0.244   6.192  -6.902    9.492      3.010    2.295       5.0   \n",
       "0.bias[1]      21.112   4.068  15.772   29.534      1.841    1.388       5.0   \n",
       "0.bias[2]      -8.665   7.806 -21.178    3.655      3.500    2.637       5.0   \n",
       "\n",
       "               ess_tail  r_hat  \n",
       "0.weight[0,0]       7.0   2.49  \n",
       "0.weight[0,1]      13.0   2.69  \n",
       "0.weight[0,2]      14.0   3.02  \n",
       "0.weight[0,3]       5.0   3.09  \n",
       "0.weight[1,0]      11.0   3.12  \n",
       "0.weight[1,1]       5.0   1.75  \n",
       "0.weight[1,2]      11.0   2.70  \n",
       "0.weight[1,3]      20.0   1.99  \n",
       "0.weight[2,0]       5.0   2.84  \n",
       "0.weight[2,1]       7.0   2.61  \n",
       "0.weight[2,2]       7.0   3.11  \n",
       "0.weight[2,3]       6.0   3.00  \n",
       "0.bias[0]          23.0   2.23  \n",
       "0.bias[1]          14.0   2.04  \n",
       "0.bias[2]          23.0   2.28  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyMC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano.tensor as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [beta, alpha]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='12000' class='' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [12000/12000 01:19<00:00 Sampling 4 chains, 1,282 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 81 seconds.\n",
      "There were 266 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 272 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 317 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 427 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=5, shape=3)\n",
    "    beta = pm.Normal('beta', mu=0, sd=5, shape=(4,3))\n",
    "    mu = pm.Deterministic('mu', alpha + pm.math.dot(X_train, beta))\n",
    "    theta = tt.nnet.softmax(mu)\n",
    "    y_hat = pm.Categorical('y_hat', p=theta, observed=y_train)\n",
    "    trace_s = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha[0]</th>\n",
       "      <td>1.109</td>\n",
       "      <td>4.913</td>\n",
       "      <td>-7.762</td>\n",
       "      <td>10.432</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.160</td>\n",
       "      <td>498.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[1]</th>\n",
       "      <td>4.529</td>\n",
       "      <td>4.465</td>\n",
       "      <td>-2.828</td>\n",
       "      <td>13.372</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.232</td>\n",
       "      <td>208.0</td>\n",
       "      <td>585.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha[2]</th>\n",
       "      <td>-5.535</td>\n",
       "      <td>4.041</td>\n",
       "      <td>-13.071</td>\n",
       "      <td>2.113</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1394.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[0,0]</th>\n",
       "      <td>1.918</td>\n",
       "      <td>3.540</td>\n",
       "      <td>-4.650</td>\n",
       "      <td>8.760</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.099</td>\n",
       "      <td>644.0</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[0,1]</th>\n",
       "      <td>0.876</td>\n",
       "      <td>3.121</td>\n",
       "      <td>-4.801</td>\n",
       "      <td>6.874</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.081</td>\n",
       "      <td>755.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[0,2]</th>\n",
       "      <td>-2.448</td>\n",
       "      <td>3.198</td>\n",
       "      <td>-8.691</td>\n",
       "      <td>3.249</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.100</td>\n",
       "      <td>513.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[1,0]</th>\n",
       "      <td>4.830</td>\n",
       "      <td>4.026</td>\n",
       "      <td>-1.753</td>\n",
       "      <td>12.752</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.159</td>\n",
       "      <td>316.0</td>\n",
       "      <td>1765.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[1,1]</th>\n",
       "      <td>-0.782</td>\n",
       "      <td>3.304</td>\n",
       "      <td>-7.066</td>\n",
       "      <td>5.150</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.123</td>\n",
       "      <td>381.0</td>\n",
       "      <td>1067.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[1,2]</th>\n",
       "      <td>-4.240</td>\n",
       "      <td>3.320</td>\n",
       "      <td>-10.575</td>\n",
       "      <td>2.185</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.079</td>\n",
       "      <td>885.0</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[2,0]</th>\n",
       "      <td>-6.674</td>\n",
       "      <td>3.582</td>\n",
       "      <td>-13.903</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.090</td>\n",
       "      <td>895.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[2,1]</th>\n",
       "      <td>0.258</td>\n",
       "      <td>3.148</td>\n",
       "      <td>-5.626</td>\n",
       "      <td>6.390</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.088</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[2,2]</th>\n",
       "      <td>6.040</td>\n",
       "      <td>3.417</td>\n",
       "      <td>0.431</td>\n",
       "      <td>12.553</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.154</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1221.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[3,0]</th>\n",
       "      <td>-3.671</td>\n",
       "      <td>4.366</td>\n",
       "      <td>-11.755</td>\n",
       "      <td>4.983</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1343.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[3,1]</th>\n",
       "      <td>-2.171</td>\n",
       "      <td>3.486</td>\n",
       "      <td>-8.698</td>\n",
       "      <td>4.602</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.084</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[3,2]</th>\n",
       "      <td>5.196</td>\n",
       "      <td>3.699</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>12.490</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.094</td>\n",
       "      <td>737.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "alpha[0]   1.109  4.913  -7.762   10.432      0.226    0.160     498.0   \n",
       "alpha[1]   4.529  4.465  -2.828   13.372      0.328    0.232     208.0   \n",
       "alpha[2]  -5.535  4.041 -13.071    2.113      0.110    0.078    1300.0   \n",
       "beta[0,0]  1.918  3.540  -4.650    8.760      0.140    0.099     644.0   \n",
       "beta[0,1]  0.876  3.121  -4.801    6.874      0.114    0.081     755.0   \n",
       "beta[0,2] -2.448  3.198  -8.691    3.249      0.141    0.100     513.0   \n",
       "beta[1,0]  4.830  4.026  -1.753   12.752      0.225    0.159     316.0   \n",
       "beta[1,1] -0.782  3.304  -7.066    5.150      0.174    0.123     381.0   \n",
       "beta[1,2] -4.240  3.320 -10.575    2.185      0.112    0.079     885.0   \n",
       "beta[2,0] -6.674  3.582 -13.903   -0.192      0.119    0.090     895.0   \n",
       "beta[2,1]  0.258  3.148  -5.626    6.390      0.093    0.088    1142.0   \n",
       "beta[2,2]  6.040  3.417   0.431   12.553      0.217    0.154     230.0   \n",
       "beta[3,0] -3.671  4.366 -11.755    4.983      0.117    0.083    1343.0   \n",
       "beta[3,1] -2.171  3.486  -8.698    4.602      0.106    0.084    1078.0   \n",
       "beta[3,2]  5.196  3.699  -1.526   12.490      0.133    0.094     737.0   \n",
       "\n",
       "           ess_tail  r_hat  \n",
       "alpha[0]      751.0   1.01  \n",
       "alpha[1]      585.0   1.02  \n",
       "alpha[2]     1394.0   1.02  \n",
       "beta[0,0]    1237.0   1.02  \n",
       "beta[0,1]    1176.0   1.01  \n",
       "beta[0,2]    1551.0   1.01  \n",
       "beta[1,0]    1765.0   1.01  \n",
       "beta[1,1]    1067.0   1.02  \n",
       "beta[1,2]    1181.0   1.01  \n",
       "beta[2,0]     987.0   1.01  \n",
       "beta[2,1]    1281.0   1.02  \n",
       "beta[2,2]    1221.0   1.02  \n",
       "beta[3,0]    2046.0   1.01  \n",
       "beta[3,1]    1430.0   1.01  \n",
       "beta[3,2]    1056.0   1.02  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(trace_s,var_names=[\"alpha\", \"beta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
