{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0beta1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.,1.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 0\n",
    "model_ctx = mx.gpu()\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 256 \n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST(train=True).transform_first(transform),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST(train=False).transform_first(transform),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1, 28, 28)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "for X,y in train_data:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference for MNIST\n",
    "\n",
    "* [Stochastic Gradient Descent](#chapter1)\n",
    "* [Stochastic Gradient Langevin Dynamics](#chapter2)\n",
    "* [Bayes By Backprop](#chapter3)\n",
    "* [Diagnostics](#chapter4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "hyper={'alpha':10.}\n",
    "in_units=(1,28,28)\n",
    "out_units=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from hamiltonian.inference.sgd import sgd\n",
    "from hamiltonian.models.softmax import lenet\n",
    "\n",
    "model=lenet(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgd(model,step_size=0.1,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=model.net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in params:\n",
    "    params[var].grad_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules re-loaded\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgd)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train loss: 0.2302, train accuracy : 0.0945\n",
      "iteration 1, train loss: 0.2301, train accuracy : 0.0949\n",
      "iteration 2, train loss: 0.2301, train accuracy : 0.0945\n",
      "iteration 3, train loss: 0.2301, train accuracy : 0.0948\n",
      "iteration 4, train loss: 0.2301, train accuracy : 0.0950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sergio/code/mxprob/benchmarks/1.-mnist-lenet.ipynb Celda 15\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sergio/code/mxprob/benchmarks/1.-mnist-lenet.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sergio/code/mxprob/benchmarks/1.-mnist-lenet.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m train_sgd:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sergio/code/mxprob/benchmarks/1.-mnist-lenet.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     par,loss\u001b[39m=\u001b[39minference\u001b[39m.\u001b[39;49mfit(epochs\u001b[39m=\u001b[39;49mnum_epochs,batch_size\u001b[39m=\u001b[39;49mbatch_size,data_loader\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sergio/code/mxprob/benchmarks/1.-mnist-lenet.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                            chain_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlenet_map.h5\u001b[39;49m\u001b[39m'\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/code/mxprob/benchmarks/1.-mnist-lenet.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     fig\u001b[39m=\u001b[39mplt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m[\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sergio/code/mxprob/benchmarks/1.-mnist-lenet.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     plt\u001b[39m.\u001b[39mplot(loss,color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m,lw\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/code/mxprob/benchmarks/../hamiltonian/inference/sgd.py:41\u001b[0m, in \u001b[0;36msgd.fit\u001b[0;34m(self, epochs, batch_size, **args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     40\u001b[0m     cumulative_loss\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mfor\u001b[39;00m j,(X_batch, y_batch) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[1;32m     42\u001b[0m         X_batch\u001b[39m=\u001b[39mX_batch\u001b[39m.\u001b[39mas_in_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx)\n\u001b[1;32m     43\u001b[0m         y_batch\u001b[39m=\u001b[39my_batch\u001b[39m.\u001b[39mas_in_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx)\n",
      "File \u001b[0;32m~/envs/mxnet2/lib/python3.8/site-packages/mxnet/gluon/data/dataloader.py:678\u001b[0m, in \u001b[0;36mDataLoader.__iter__.<locals>.same_process_iter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msame_process_iter\u001b[39m():\n\u001b[1;32m    677\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_sampler:\n\u001b[0;32m--> 678\u001b[0m         ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batchify_fn([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m batch])\n\u001b[1;32m    679\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m             ret \u001b[39m=\u001b[39m _as_in_context(ret, context\u001b[39m.\u001b[39mcpu_pinned(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_device_id))\n",
      "File \u001b[0;32m~/envs/mxnet2/lib/python3.8/site-packages/mxnet/gluon/data/dataloader.py:678\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msame_process_iter\u001b[39m():\n\u001b[1;32m    677\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_sampler:\n\u001b[0;32m--> 678\u001b[0m         ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batchify_fn([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m batch])\n\u001b[1;32m    679\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m             ret \u001b[39m=\u001b[39m _as_in_context(ret, context\u001b[39m.\u001b[39mcpu_pinned(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_device_id))\n",
      "File \u001b[0;32m~/envs/mxnet2/lib/python3.8/site-packages/mxnet/gluon/data/dataset.py:234\u001b[0m, in \u001b[0;36m_LazyTransformDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m--> 234\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data[idx]\n\u001b[1;32m    235\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    236\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fn(\u001b[39m*\u001b[39mitem)\n",
      "File \u001b[0;32m~/envs/mxnet2/lib/python3.8/site-packages/mxnet/gluon/data/dataset.py:437\u001b[0m, in \u001b[0;36m_DownloadedDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label[idx])\n\u001b[0;32m--> 437\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data[idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label[idx]\n",
      "File \u001b[0;32m~/envs/mxnet2/lib/python3.8/site-packages/mxnet/numpy/multiarray.py:824\u001b[0m, in \u001b[0;36mndarray.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, integer_types):\n\u001b[1;32m    823\u001b[0m     \u001b[39m# Equivalent to isinstance(key, integer_types) case in numpy/_symbol.py\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39m>\u001b[39m shape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    825\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m    826\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is out of bounds for axis 0 with size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    827\u001b[0m                 key, shape[\u001b[39m0\u001b[39m]))\n\u001b[1;32m    828\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_at(key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sgd=True\n",
    "num_epochs=10\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if train_sgd:\n",
    "    par,loss=inference.fit(epochs=num_epochs,batch_size=batch_size,data_loader=train_data,\n",
    "                           chain_name='lenet_map.h5',verbose=True)\n",
    "\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss,color='blue',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGD Lenet MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgd_lenet.pdf', bbox_inches='tight')\n",
    "else:\n",
    "    map_estimate=h5py.File('lenet_map.h5','r')\n",
    "    par={var:map_estimate[var][:] for var in map_estimate.keys()}\n",
    "    map_estimate.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.weight': Parameter (shape=(6, 1, 5, 5), dtype=<class 'numpy.float32'>),\n",
       " '0.bias': Parameter (shape=(6,), dtype=<class 'numpy.float32'>),\n",
       " '2.weight': Parameter (shape=(16, 6, 5, 5), dtype=<class 'numpy.float32'>),\n",
       " '2.bias': Parameter (shape=(16,), dtype=<class 'numpy.float32'>),\n",
       " '4.weight': Parameter (shape=(120, 400), dtype=float32),\n",
       " '4.bias': Parameter (shape=(120,), dtype=float32),\n",
       " '5.weight': Parameter (shape=(84, 120), dtype=float32),\n",
       " '5.bias': Parameter (shape=(84,), dtype=float32),\n",
       " '6.weight': Parameter (shape=(10, 84), dtype=float32),\n",
       " '6.bias': Parameter (shape=(10,), dtype=float32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimate=h5py.File('lenet_map.h5','r')\n",
    "par={var:map_estimate[var][:] for var in map_estimate.keys()}\n",
    "map_estimate.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(par,batch_size=batch_size,num_samples=100,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=np.quantile(total_samples,.9,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       980\n",
      "           1       0.00      0.00      0.00      1135\n",
      "           2       0.00      0.00      0.00      1032\n",
      "           3       0.00      0.00      0.00      1010\n",
      "           4       0.00      0.00      0.00       982\n",
      "           5       0.00      0.00      0.00       892\n",
      "           6       0.00      0.00      0.00       958\n",
      "           7       0.00      0.00      0.00      1028\n",
      "           8       0.10      0.25      0.14       974\n",
      "           9       0.10      0.75      0.18      1009\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.02      0.10      0.03     10000\n",
      "weighted avg       0.02      0.10      0.03     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Langevin Dynamics <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.inference.sgld import sgld\n",
    "from hamiltonian.models.softmax import lenet\n",
    "\n",
    "model=lenet(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgld(model,par,step_size=0.01,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgld)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_sgld=False\n",
    "num_epochs=250\n",
    "\n",
    "if train_sgld:\n",
    "    loss,posterior_samples=inference.sample(epochs=num_epochs,batch_size=batch_size,\n",
    "                                data_loader=train_data,\n",
    "                                verbose=True,chain_name='lenet_posterior.h5')\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 360\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss[0],color='blue',lw=3)\n",
    "    plt.plot(loss[1],color='red',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGLD Lenet MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgld_lenet.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "posterior_samples=h5py.File('lenet_posterior.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(posterior_samples,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "posterior_samples\n",
    "\n",
    "y_hat=np.quantile(total_samples,.5,axis=0)\n",
    "\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "samples={var:posterior_samples[var] for var in posterior_samples.keys()}\n",
    "samples={var:np.swapaxes(samples[var],0,1) for var in model.par}\n",
    "r_hat_estimate = lambda samples : tfp.mcmc.diagnostic.potential_scale_reduction(samples, independent_chain_ndims=1,split_chains=False).numpy()\n",
    "rhat = {var:r_hat_estimate(samples[var]) for var in model.par}\n",
    "median_rhat_flat={var:np.median(rhat[var]) for var in rhat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = rhat.keys(), rhat.values()\n",
    "flatten_data=list()\n",
    "for d in data:\n",
    "    flatten_data.append(d.reshape(-1))\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 360\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.boxplot(flatten_data)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.title('Rhat')\n",
    "plt.savefig('rhat_nonhierarchical_lenet.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_estimate = lambda samples : tfp.mcmc.diagnostic.effective_sample_size(samples, filter_beyond_positive_pairs=False,cross_chain_dims=1).numpy()\n",
    "ess = {var:ess_estimate(samples[var]) for var in model.par}\n",
    "median_ess_flat={var:np.median(ess[var]) for var in ess}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = ess.keys(), ess.values()\n",
    "flatten_data=list()\n",
    "for d in data:\n",
    "    flatten_data.append(d.reshape(-1))\n",
    "plt.rcParams['figure.dpi'] = 360\n",
    "sns.set_style(\"whitegrid\")    \n",
    "plt.boxplot(flatten_data)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.title('ESS')\n",
    "plt.savefig('ess_nonhierarchical_lenet.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.utils.psis import *\n",
    "\n",
    "loo,loos,ks=psisloo(log_like)\n",
    "max_ks=max(ks[~ np.isinf(ks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_ks_1=np.sum(ks>1)\n",
    "flat_ks_7_1=np.sum(np.logical_and(ks>0.7,ks<1))\n",
    "flat_ks_5_7=np.sum(np.logical_and(ks>0.5,ks<0.7))\n",
    "flat_ks_5=np.sum(ks<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Lenet <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.inference.sgld import hierarchical_sgld\n",
    "from hamiltonian.models.softmax import hierarchical_lenet\n",
    "\n",
    "model=hierarchical_lenet(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=hierarchical_sgld(model,par,step_size=0.001,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgld)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "train_sgld=True\n",
    "num_epochs=250\n",
    "\n",
    "if train_sgld:\n",
    "    loss,posterior_samples=inference.sample(epochs=num_epochs,batch_size=batch_size,\n",
    "                                data_loader=train_data,\n",
    "                                verbose=True,chain_name='hierarchical_lenet_posterior.h5')\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 360\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss[0],color='blue',lw=3)\n",
    "    plt.plot(loss[1],color='red',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGLD Hierarchical Lenet MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgld_hierarchical_lenet.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "posterior_samples=h5py.File('hierarchical_lenet_posterior.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(posterior_samples,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_hat=np.quantile(total_samples,.5,axis=0)\n",
    "\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "samples={var:posterior_samples[var] for var in posterior_samples.keys()}\n",
    "samples={var:np.swapaxes(samples[var],0,1) for var in model.par}\n",
    "r_hat_estimate = lambda samples : tfp.mcmc.diagnostic.potential_scale_reduction(samples, independent_chain_ndims=1,split_chains=True).numpy()\n",
    "rhat = {var:r_hat_estimate(samples[var]) for var in model.par}\n",
    "median_rhat_hierarchical={var:np.median(rhat[var]) for var in rhat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = rhat.keys(), rhat.values()\n",
    "flatten_data=list()\n",
    "for d in data:\n",
    "    flatten_data.append(d.reshape(-1))\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 360\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.boxplot(flatten_data)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.title('Rhat')\n",
    "plt.savefig('rhat_hierarchical_lenet.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_estimate = lambda samples : tfp.mcmc.diagnostic.effective_sample_size(samples, filter_beyond_positive_pairs=False,cross_chain_dims=1).numpy()\n",
    "ess = {var:ess_estimate(samples[var]) for var in model.par}\n",
    "median_ess_hierarchical={var:np.median(ess[var]) for var in ess}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = ess.keys(), ess.values()\n",
    "flatten_data=list()\n",
    "for d in data:\n",
    "    flatten_data.append(d.reshape(-1))\n",
    "plt.rcParams['figure.dpi'] = 360\n",
    "sns.set_style(\"whitegrid\")    \n",
    "plt.boxplot(flatten_data)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.title('ESS')\n",
    "plt.savefig('ess_hierarchical_lenet.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.utils.psis import *\n",
    "\n",
    "loo,loos,ks=psisloo(log_like)\n",
    "max_ks=max(ks[~ np.isinf(ks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_ks_1=np.sum(ks>1)\n",
    "hierarchical_ks_7_1=np.sum(np.logical_and(ks>0.7,ks<1))\n",
    "hierarchical_ks_5_7=np.sum(np.logical_and(ks>0.5,ks<0.7))\n",
    "hierarchical_ks_5=np.sum(ks<0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hierarchical=[hierarchical_ks_1,hierarchical_ks_7_1,hierarchical_ks_5_7,hierarchical_ks_5]\n",
    "flat=[flat_ks_1,flat_ks_7_1,flat_ks_5_7,flat_ks_5]\n",
    "index = ['k>1', '0.7<k<1', '0.5<k<0.7','k<0.5']\n",
    "df = pd.DataFrame({'hierarchical': hierarchical,\n",
    "                   'non-hierarchical': flat}, index=index)\n",
    "ax = df.plot.bar(rot=0)\n",
    "plt.title('Pareto K shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical=median_rhat_hierarchical.values()\n",
    "flat=median_rhat_flat.values()\n",
    "index = median_rhat_flat.keys()\n",
    "df = pd.DataFrame({'hierarchical': hierarchical,\n",
    "                   'non-hierarchical': flat}, index=index)\n",
    "ax = df.plot.bar(rot=0)\n",
    "plt.title('Potential Scale Reduction (Rhat)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical=median_ess_hierarchical.values()\n",
    "flat=median_ess_flat.values()\n",
    "index = median_ess_flat.keys()\n",
    "df = pd.DataFrame({'hierarchical': hierarchical,\n",
    "                   'non-hierarchical': flat}, index=index)\n",
    "ax = df.plot.bar(rot=0)\n",
    "plt.title('ESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxnet2",
   "language": "python",
   "name": "mxnet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
