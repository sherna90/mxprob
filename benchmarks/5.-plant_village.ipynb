{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize([150,150]),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path='/media/sergio/Backup/data/PlantVillage-Dataset/data_distribution_for_SVM/train'\n",
    "testing_path='/media/sergio/Backup/data/PlantVillage-Dataset/data_distribution_for_SVM/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "model_ctx = mx.gpu()\n",
    "\n",
    "num_workers = 2\n",
    "batch_size = 32 \n",
    "\n",
    "train_data = mx.gluon.data.vision.datasets.ImageFolderDataset(training_path).transform_first(transform)\n",
    "test_data = mx.gluon.data.vision.datasets.ImageFolderDataset(testing_path).transform_first(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = mx.gluon.data.DataLoader(train_data, batch_size, shuffle=True, num_workers=num_workers)\n",
    "valid_data_loader = mx.gluon.data.DataLoader(test_data, batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 150, 150)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for X,y in train_data_loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference for Plant Village\n",
    "\n",
    "* [Stochastic Gradient Descent](#chapter1)\n",
    "* [Stochastic Gradient Langevin Dynamics](#chapter2)\n",
    "* [Bayes By Backprop](#chapter3)\n",
    "* [Diagnostics](#chapter4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "hyper={'alpha':10.}\n",
    "in_units=(3,150,150)\n",
    "out_units=38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from hamiltonian.inference.sgd import sgd\n",
    "from hamiltonian.models.softmax import vgg_softmax\n",
    "\n",
    "model=vgg_softmax(hyper,in_units,out_units,n_layers=16,ctx=model_ctx)\n",
    "inference=sgd(model,model.par,step_size=0.1,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no modules loaded yet\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.vgg_softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgd)\n",
    "    importlib.reload(hamiltonian.inference.base)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:53<1:27:36, 53.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.7592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 3/100 [08:32<4:36:15, 170.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-89c3ad3c28d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_sgd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/hamiltonian_mxnet/benchmarks/../hamiltonian/inference/sgd.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, batch_size, **args)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#calculo de derivadas parciales de la funcion segun sus parametros. por retropropagacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mcumulative_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mloss_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcumulative_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/mxnet/python/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2627\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2630\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/mxnet/python/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2605\u001b[0m         \"\"\"\n\u001b[1;32m   2606\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         check_call(_LIB.MXNDArraySyncCopyToCPU(\n\u001b[0m\u001b[1;32m   2608\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sgd=True\n",
    "num_epochs=100\n",
    "\n",
    "if train_sgd:\n",
    "    par,loss=inference.fit(epochs=num_epochs,batch_size=batch_size,data_loader=train_data_loader,verbose=True)\n",
    "\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss,color='blue',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGD VGG Plant Village', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgd_vgg.pdf', bbox_inches='tight')\n",
    "    model.net.save_parameters('../scripts/results/vgg/vgg_sgd_'+str(num_epochs)+'_epochs.params')\n",
    "else:\n",
    "    model.net.load_parameters('../scripts/results/vgg/vgg_sgd_'+str(num_epochs)+'_epochs.params',ctx=model_ctx)\n",
    "    par=dict()\n",
    "    for name,gluon_par in model.net.collect_params().items():\n",
    "        par.update({name:gluon_par.data()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.weight': Parameter (shape=(6, 1, 5, 5), dtype=<class 'numpy.float32'>),\n",
       " '0.bias': Parameter (shape=(6,), dtype=<class 'numpy.float32'>),\n",
       " '2.weight': Parameter (shape=(16, 6, 5, 5), dtype=<class 'numpy.float32'>),\n",
       " '2.bias': Parameter (shape=(16,), dtype=<class 'numpy.float32'>),\n",
       " '4.weight': Parameter (shape=(120, 400), dtype=float32),\n",
       " '4.bias': Parameter (shape=(120,), dtype=float32),\n",
       " '5.weight': Parameter (shape=(84, 120), dtype=float32),\n",
       " '5.bias': Parameter (shape=(84,), dtype=float32),\n",
       " '6.weight': Parameter (shape=(10, 84), dtype=float32),\n",
       " '6.bias': Parameter (shape=(10,), dtype=float32)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(par,batch_size=batch_size,num_samples=100,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=np.quantile(total_samples,.9,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       979\n",
      "           1       1.00      0.99      0.99      1133\n",
      "           2       0.99      0.99      0.99      1030\n",
      "           3       0.99      0.99      0.99      1008\n",
      "           4       0.99      0.98      0.99       980\n",
      "           5       0.99      0.99      0.99       890\n",
      "           6       0.99      0.99      0.99       956\n",
      "           7       0.98      0.98      0.98      1027\n",
      "           8       0.99      0.98      0.99       973\n",
      "           9       0.96      0.99      0.98      1008\n",
      "\n",
      "    accuracy                           0.99      9984\n",
      "   macro avg       0.99      0.99      0.99      9984\n",
      "weighted avg       0.99      0.99      0.99      9984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Langevin Dynamics <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.inference.sgld import sgld\n",
    "from hamiltonian.models.softmax import vgg_softmax\n",
    "\n",
    "model=vgg_softmax(hyper,in_units,out_units,n_layers=16,ctx=model_ctx)\n",
    "inference=sgld(model,model.par,step_size=0.1,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules re-loaded\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgld)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "train_sgld=False\n",
    "num_epochs=250\n",
    "\n",
    "if train_sgld:\n",
    "    loss,posterior_samples=inference.sample(epochs=num_epochs,batch_size=batch_size,\n",
    "                                data_loader=train_data,\n",
    "                                verbose=True,chain_name='chain_nonhierarchical')\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 360\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss[0],color='blue',lw=3)\n",
    "    plt.plot(loss[1],color='red',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGLD Lenet MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgld_lenet.pdf', bbox_inches='tight')\n",
    "else:\n",
    "    chain1=glob.glob(\"../scripts/results/vgg/chain_nonhierarchical_0_1_sgld*\")\n",
    "    chain2=glob.glob(\"../scripts/results/vgg/chain_nonhierarchical_0_sgld*\")\n",
    "    posterior_samples=[chain1,chain2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_flat=[item for sublist in posterior_samples for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(posterior_samples_flat,5,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       980\n",
      "           1       1.00      0.96      0.98      1135\n",
      "           2       0.98      0.90      0.94      1032\n",
      "           3       0.96      0.93      0.95      1010\n",
      "           4       0.98      0.92      0.95       982\n",
      "           5       0.95      0.94      0.94       892\n",
      "           6       0.94      0.99      0.96       958\n",
      "           7       0.94      0.96      0.95      1028\n",
      "           8       0.89      0.98      0.93       974\n",
      "           9       0.89      1.00      0.94      1009\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "posterior_samples\n",
    "\n",
    "y_hat=np.quantile(total_samples,.9,axis=0)\n",
    "\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean f-1 : 0.9748208806697447, std f-1 : 0.024564142639161658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score=[]\n",
    "for q in np.arange(.1,.9,.1):\n",
    "    y_hat=np.quantile(total_samples,q,axis=0)\n",
    "    score.append(f1_score(np.int32(total_labels),np.int32(y_hat), average='macro'))\n",
    "print('mean f-1 : {0}, std f-1 : {1}'.format(np.mean(score),2*np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "posterior_samples_multiple_chains=inference.posterior_diagnostics(posterior_samples)\n",
    "datasets=[az.convert_to_inference_data(sample) for sample in posterior_samples_multiple_chains]\n",
    "dataset = az.concat(datasets, dim=\"chain\")\n",
    "mean_r_hat_values={var:float(az.rhat(dataset)[var].mean().data) for var in model.par}\n",
    "mean_ess_values={var:float(az.ess(dataset)[var].mean().data) for var in model.par}\n",
    "mean_mcse_values={var:float(az.mcse(dataset)[var].mean().data) for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.weight[0,0,0,0]</th>\n",
       "      <td>-1.612</td>\n",
       "      <td>1.635</td>\n",
       "      <td>-3.627</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.787</td>\n",
       "      <td>3.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[0,0,0,1]</th>\n",
       "      <td>-0.462</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-1.657</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.254</td>\n",
       "      <td>3.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[0,0,0,2]</th>\n",
       "      <td>0.110</td>\n",
       "      <td>1.375</td>\n",
       "      <td>-1.645</td>\n",
       "      <td>2.617</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.679</td>\n",
       "      <td>3.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[0,0,0,3]</th>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.597</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.171</td>\n",
       "      <td>10.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.weight[0,0,0,4]</th>\n",
       "      <td>-0.734</td>\n",
       "      <td>0.995</td>\n",
       "      <td>-2.361</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.450</td>\n",
       "      <td>3.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.bias[5]</th>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.059</td>\n",
       "      <td>30.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.bias[6]</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.653</td>\n",
       "      <td>-1.156</td>\n",
       "      <td>1.064</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.319</td>\n",
       "      <td>3.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.bias[7]</th>\n",
       "      <td>0.034</td>\n",
       "      <td>0.569</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>1.299</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.054</td>\n",
       "      <td>52.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.bias[8]</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.814</td>\n",
       "      <td>-1.618</td>\n",
       "      <td>1.821</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.236</td>\n",
       "      <td>7.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.bias[9]</th>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.841</td>\n",
       "      <td>-1.244</td>\n",
       "      <td>1.586</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61706 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
       "0.weight[0,0,0,0] -1.612  1.635  -3.627    1.164      0.985    0.787   \n",
       "0.weight[0,0,0,1] -0.462  0.614  -1.657    0.661      0.328    0.254   \n",
       "0.weight[0,0,0,2]  0.110  1.375  -1.645    2.617      0.845    0.679   \n",
       "0.weight[0,0,0,3] -0.200  0.597  -1.099    0.912      0.230    0.171   \n",
       "0.weight[0,0,0,4] -0.734  0.995  -2.361    0.737      0.571    0.450   \n",
       "...                  ...    ...     ...      ...        ...      ...   \n",
       "6.bias[5]         -0.087  0.559  -0.975    0.985      0.083    0.059   \n",
       "6.bias[6]         -0.083  0.653  -1.156    1.064      0.397    0.319   \n",
       "6.bias[7]          0.034  0.569  -0.937    1.299      0.076    0.054   \n",
       "6.bias[8]          0.088  0.814  -1.618    1.821      0.319    0.236   \n",
       "6.bias[9]         -0.079  0.841  -1.244    1.586      0.389    0.294   \n",
       "\n",
       "                   ess_bulk  ess_tail  r_hat  \n",
       "0.weight[0,0,0,0]       3.0     246.0   1.77  \n",
       "0.weight[0,0,0,1]       3.0     166.0   1.59  \n",
       "0.weight[0,0,0,2]       3.0     144.0   1.80  \n",
       "0.weight[0,0,0,3]      10.0     120.0   1.23  \n",
       "0.weight[0,0,0,4]       3.0     196.0   1.60  \n",
       "...                     ...       ...    ...  \n",
       "6.bias[5]              30.0     404.0   1.05  \n",
       "6.bias[6]               3.0     133.0   1.80  \n",
       "6.bias[7]              52.0     282.0   1.05  \n",
       "6.bias[8]               7.0     331.0   1.24  \n",
       "6.bias[9]               5.0      96.0   1.33  \n",
       "\n",
       "[61706 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.weight': 1.4433263528009008, '0.bias': 1.648076802874187, '2.weight': 1.4908360672675494, '2.bias': 1.2081504842012865, '4.weight': 1.480469265487481, '4.bias': 1.4979961639966641, '5.weight': 1.4818768559638393, '5.bias': 1.4850130190063169, '6.weight': 1.4859056781765567, '6.bias': 1.2062475720278303}\n"
     ]
    }
   ],
   "source": [
    "print(mean_r_hat_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.weight': 56.85196954744781, '0.bias': 3.7896113973529553, '2.weight': 57.84407245511651, '2.bias': 115.52266689594174, '4.weight': 61.9930606333923, '4.bias': 68.92474001737575, '5.weight': 60.49414603670665, '5.bias': 73.17804413230593, '6.weight': 59.562982006241995, '6.bias': 146.29305834568964}\n"
     ]
    }
   ],
   "source": [
    "print(mean_ess_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0.weight': 0.5342363969670794, '0.bias': 0.47947446717104025, '2.weight': 0.6957483945243773, '2.bias': 0.24764692875686375, '4.weight': 0.7057690437286156, '4.bias': 0.6422108762806051, '5.weight': 0.7041383246810111, '5.bias': 0.594246269900714, '6.weight': 0.634655255318478, '6.bias': 0.18597643886440535}\n"
     ]
    }
   ],
   "source": [
    "print(mean_mcse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.psis import *\n",
    "\n",
    "loo,loos,ks=psisloo(-log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean f-1 : 0.579054715975662, std f-1 : 0.11209524379518138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score=[]\n",
    "for q in np.arange(.1,.9,.1):\n",
    "    y_hat=np.quantile(total_samples,q,axis=0)\n",
    "    score.append(f1_score(np.int32(total_labels),np.int32(y_hat), sample_weight=loos,average='weighted'))\n",
    "print('mean f-1 : {0}, std f-1 : {1}'.format(np.mean(score),2*np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
