{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.,1.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "model_ctx = mx.gpu()\n",
    "\n",
    "num_workers = 2\n",
    "batch_size = 64 \n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST(train=True).transform_first(transform),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST(train=False).transform_first(transform),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 28, 28)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "for X,y in train_data:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference for MNIST\n",
    "\n",
    "* [Stochastic Gradient Descent](#chapter1)\n",
    "* [Stochastic Gradient Langevin Dynamics](#chapter2)\n",
    "* [Bayes By Backprop](#chapter3)\n",
    "* [Diagnostics](#chapter4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "hyper={'alpha':10.}\n",
    "in_units=(28,28)\n",
    "out_units=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from hamiltonian.inference.sgd import sgd\n",
    "from hamiltonian.models.softmax import softmax\n",
    "\n",
    "model=softmax(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgd(model,model.par,step_size=0.001,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules re-loaded\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgd)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgd=False\n",
    "num_epochs=100\n",
    "if train_sgd:\n",
    "    par,loss=inference.fit(epochs=num_epochs,batch_size=batch_size,data_loader=train_data,verbose=True)\n",
    "\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss,color='blue',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGD Softmax MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgd_softmax.pdf', bbox_inches='tight')\n",
    "    model.net.save_parameters('softmax_sgd_'+str(num_epochs)+'_epochs.params')\n",
    "else:\n",
    "    model.net.load_parameters('softmax_sgd_'+str(num_epochs)+'_epochs.params',ctx=model_ctx)\n",
    "    par=dict()\n",
    "    for name,gluon_par in model.net.collect_params().items():\n",
    "        par.update({name:gluon_par.data()})\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       979\n",
      "           1       0.97      0.97      0.97      1133\n",
      "           2       0.92      0.88      0.90      1030\n",
      "           3       0.89      0.89      0.89      1008\n",
      "           4       0.88      0.92      0.90       980\n",
      "           5       0.83      0.86      0.85       890\n",
      "           6       0.89      0.93      0.91       956\n",
      "           7       0.91      0.91      0.91      1027\n",
      "           8       0.85      0.84      0.85       973\n",
      "           9       0.93      0.85      0.89      1008\n",
      "\n",
      "    accuracy                           0.90      9984\n",
      "   macro avg       0.90      0.90      0.90      9984\n",
      "weighted avg       0.91      0.90      0.90      9984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "total_samples,total_labels,log_like=inference.predict(par,batch_size=batch_size,num_samples=10,data_loader=val_data)\n",
    "y_hat=np.quantile(total_samples,.5,axis=0)\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': Parameter (shape=(10, 784), dtype=float32),\n",
       " '1.bias': Parameter (shape=(10,), dtype=float32)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Langevin Dynamics <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.inference.sgld import sgld\n",
    "\n",
    "model=softmax(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgld(model,model.par,step_size=0.01,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules re-loaded\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgld)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "train_sgld=False\n",
    "num_epochs=100\n",
    "\n",
    "if train_sgld:\n",
    "    loss,posterior_samples=inference.sample(epochs=num_epochs,batch_size=batch_size,\n",
    "                                data_loader=train_data,\n",
    "                                verbose=True,chain_name='chain_nonhierarchical')\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 360\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss[0],color='blue',lw=3)\n",
    "    plt.plot(loss[1],color='red',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGLD Softmax MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgld_softmax.pdf', bbox_inches='tight')\n",
    "else:\n",
    "    chain1=glob.glob(\"../scripts/results/softmax/chain_nonhierarchical_0_1_sgld*\")\n",
    "    chain2=glob.glob(\"../scripts/results/softmax/chain_nonhierarchical_0_sgld*\")\n",
    "    posterior_samples=[chain1,chain2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_flat=[item for sublist in posterior_samples for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(posterior_samples_flat,5,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       980\n",
      "           1       0.97      0.98      0.97      1135\n",
      "           2       0.93      0.89      0.91      1032\n",
      "           3       0.89      0.90      0.90      1010\n",
      "           4       0.92      0.93      0.93       982\n",
      "           5       0.86      0.88      0.87       892\n",
      "           6       0.94      0.95      0.95       958\n",
      "           7       0.92      0.93      0.92      1028\n",
      "           8       0.87      0.87      0.87       974\n",
      "           9       0.93      0.89      0.91      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_hat=np.quantile(total_samples,.5,axis=0)\n",
    "\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "posterior_samples_multiple_chains=inference.posterior_diagnostics(posterior_samples)\n",
    "datasets=[az.convert_to_inference_data(sample) for sample in posterior_samples_multiple_chains]\n",
    "dataset = az.concat(datasets, dim=\"chain\")\n",
    "mean_r_hat_values={var:float(az.rhat(dataset)[var].mean().data) for var in model.par}\n",
    "mean_ess_values={var:float(az.ess(dataset)[var].mean().data) for var in model.par}\n",
    "mean_mcse_values={var:float(az.mcse(dataset)[var].mean().data) for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.weight[0,0]</th>\n",
       "      <td>-0.496</td>\n",
       "      <td>1.478</td>\n",
       "      <td>-2.993</td>\n",
       "      <td>1.342</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.687</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,1]</th>\n",
       "      <td>-2.072</td>\n",
       "      <td>0.963</td>\n",
       "      <td>-3.561</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.056</td>\n",
       "      <td>151.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,2]</th>\n",
       "      <td>-2.137</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-3.061</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.147</td>\n",
       "      <td>11.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,3]</th>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.714</td>\n",
       "      <td>-1.563</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.144</td>\n",
       "      <td>59.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,4]</th>\n",
       "      <td>0.265</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-1.162</td>\n",
       "      <td>1.501</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.411</td>\n",
       "      <td>3.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[5]</th>\n",
       "      <td>3.268</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.617</td>\n",
       "      <td>4.547</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.239</td>\n",
       "      <td>5.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[6]</th>\n",
       "      <td>0.029</td>\n",
       "      <td>0.477</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.027</td>\n",
       "      <td>143.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[7]</th>\n",
       "      <td>1.688</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.899</td>\n",
       "      <td>2.364</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.032</td>\n",
       "      <td>161.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[8]</th>\n",
       "      <td>-2.649</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-3.663</td>\n",
       "      <td>-1.877</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.038</td>\n",
       "      <td>97.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[9]</th>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.585</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.144</td>\n",
       "      <td>13.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7850 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "1.weight[0,0] -0.496  1.478  -2.993    1.342      0.867    0.687       3.0   \n",
       "1.weight[0,1] -2.072  0.963  -3.561   -0.288      0.078    0.056     151.0   \n",
       "1.weight[0,2] -2.137  0.704  -3.061   -0.540      0.149    0.147      11.0   \n",
       "1.weight[0,3] -0.169  0.714  -1.563    1.093      0.158    0.144      59.0   \n",
       "1.weight[0,4]  0.265  0.833  -1.162    1.501      0.511    0.411       3.0   \n",
       "...              ...    ...     ...      ...        ...      ...       ...   \n",
       "1.bias[5]      3.268  0.810   1.617    4.547      0.283    0.239       5.0   \n",
       "1.bias[6]      0.029  0.477  -0.916    0.814      0.038    0.027     143.0   \n",
       "1.bias[7]      1.688  0.464   0.899    2.364      0.045    0.032     161.0   \n",
       "1.bias[8]     -2.649  0.514  -3.663   -1.877      0.054    0.038      97.0   \n",
       "1.bias[9]     -0.167  0.585  -0.967    1.032      0.197    0.144      13.0   \n",
       "\n",
       "               ess_tail  r_hat  \n",
       "1.weight[0,0]      60.0   1.83  \n",
       "1.weight[0,1]     132.0   1.18  \n",
       "1.weight[0,2]      62.0   1.14  \n",
       "1.weight[0,3]     222.0   1.13  \n",
       "1.weight[0,4]     107.0   1.83  \n",
       "...                 ...    ...  \n",
       "1.bias[5]          98.0   1.32  \n",
       "1.bias[6]          74.0   1.03  \n",
       "1.bias[7]          47.0   1.12  \n",
       "1.bias[8]         136.0   1.06  \n",
       "1.bias[9]         176.0   1.15  \n",
       "\n",
       "[7850 rows x 9 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.weight': 1.453861862862687, '1.bias': 1.1607318528841408}\n"
     ]
    }
   ],
   "source": [
    "print(mean_r_hat_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 30.12714771000977, '1.bias': 64.02426545788761}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_ess_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 0.4712754299075291, '1.bias': 0.13311773795785092}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_mcse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.psis import *\n",
    "\n",
    "loo,loos,ks=psisloo(log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks[ks>1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6286"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ks>0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Softmax <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.models.softmax import hierarchical_softmax\n",
    "from hamiltonian.inference.sgld import sgld\n",
    "\n",
    "model=hierarchical_softmax(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgld(model,model.par,step_size=0.01,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgld=False\n",
    "num_epochs=100\n",
    "\n",
    "if train_sgld:\n",
    "    loss,posterior_samples=inference.sample(epochs=num_epochs,batch_size=batch_size,\n",
    "                                data_loader=train_data,\n",
    "                                verbose=True,chain_name='chain_hierarchical')\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 360\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss[0],color='blue',lw=3)\n",
    "    plt.plot(loss[1],color='red',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGLD Hierarchical Softmax MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgld_hierarchical_softmax.pdf', bbox_inches='tight')\n",
    "else:\n",
    "    chain1=glob.glob(\"../scripts/results/softmax/chain_hierarchical_0_1_sgld*\")\n",
    "    chain2=glob.glob(\"../scripts/results/softmax/chain_hierarchical_0_sgld*\")\n",
    "    posterior_samples=[chain1,chain2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(posterior_samples_flat,5,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       980\n",
      "           1       0.98      0.97      0.98      1135\n",
      "           2       0.93      0.90      0.91      1032\n",
      "           3       0.89      0.90      0.90      1010\n",
      "           4       0.92      0.93      0.93       982\n",
      "           5       0.86      0.87      0.87       892\n",
      "           6       0.92      0.94      0.93       958\n",
      "           7       0.91      0.93      0.92      1028\n",
      "           8       0.87      0.87      0.87       974\n",
      "           9       0.93      0.89      0.91      1009\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_hat=np.quantile(total_samples,.5,axis=0)\n",
    "\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_flat=[item for sublist in posterior_samples for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "datasets=[az.convert_to_inference_data(sample) for sample in posterior_samples_multiple_chains]\n",
    "dataset = az.concat(datasets, dim=\"chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.weight[0,0]</th>\n",
       "      <td>-0.496</td>\n",
       "      <td>1.478</td>\n",
       "      <td>-2.993</td>\n",
       "      <td>1.342</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.687</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,1]</th>\n",
       "      <td>-2.072</td>\n",
       "      <td>0.963</td>\n",
       "      <td>-3.561</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.056</td>\n",
       "      <td>151.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,2]</th>\n",
       "      <td>-2.137</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-3.061</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.147</td>\n",
       "      <td>11.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,3]</th>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.714</td>\n",
       "      <td>-1.563</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.144</td>\n",
       "      <td>59.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,4]</th>\n",
       "      <td>0.265</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-1.162</td>\n",
       "      <td>1.501</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.411</td>\n",
       "      <td>3.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[5]</th>\n",
       "      <td>3.268</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.617</td>\n",
       "      <td>4.547</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.239</td>\n",
       "      <td>5.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[6]</th>\n",
       "      <td>0.029</td>\n",
       "      <td>0.477</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.027</td>\n",
       "      <td>143.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[7]</th>\n",
       "      <td>1.688</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.899</td>\n",
       "      <td>2.364</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.032</td>\n",
       "      <td>161.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[8]</th>\n",
       "      <td>-2.649</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-3.663</td>\n",
       "      <td>-1.877</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.038</td>\n",
       "      <td>97.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[9]</th>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.585</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.144</td>\n",
       "      <td>13.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7850 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "1.weight[0,0] -0.496  1.478  -2.993    1.342      0.867    0.687       3.0   \n",
       "1.weight[0,1] -2.072  0.963  -3.561   -0.288      0.078    0.056     151.0   \n",
       "1.weight[0,2] -2.137  0.704  -3.061   -0.540      0.149    0.147      11.0   \n",
       "1.weight[0,3] -0.169  0.714  -1.563    1.093      0.158    0.144      59.0   \n",
       "1.weight[0,4]  0.265  0.833  -1.162    1.501      0.511    0.411       3.0   \n",
       "...              ...    ...     ...      ...        ...      ...       ...   \n",
       "1.bias[5]      3.268  0.810   1.617    4.547      0.283    0.239       5.0   \n",
       "1.bias[6]      0.029  0.477  -0.916    0.814      0.038    0.027     143.0   \n",
       "1.bias[7]      1.688  0.464   0.899    2.364      0.045    0.032     161.0   \n",
       "1.bias[8]     -2.649  0.514  -3.663   -1.877      0.054    0.038      97.0   \n",
       "1.bias[9]     -0.167  0.585  -0.967    1.032      0.197    0.144      13.0   \n",
       "\n",
       "               ess_tail  r_hat  \n",
       "1.weight[0,0]      60.0   1.83  \n",
       "1.weight[0,1]     132.0   1.18  \n",
       "1.weight[0,2]      62.0   1.14  \n",
       "1.weight[0,3]     222.0   1.13  \n",
       "1.weight[0,4]     107.0   1.83  \n",
       "...                 ...    ...  \n",
       "1.bias[5]          98.0   1.32  \n",
       "1.bias[6]          74.0   1.03  \n",
       "1.bias[7]          47.0   1.12  \n",
       "1.bias[8]         136.0   1.06  \n",
       "1.bias[9]         176.0   1.15  \n",
       "\n",
       "[7850 rows x 9 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.weight': 1.453861862862687, '1.bias': 1.1607318528841408}\n"
     ]
    }
   ],
   "source": [
    "mean_r_hat_values={var:float(az.rhat(dataset)[var].mean().data) for var in model.par}\n",
    "print(mean_r_hat_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.weight': 30.12714771000977, '1.bias': 64.02426545788761}\n"
     ]
    }
   ],
   "source": [
    "mean_ess_values={var:float(az.ess(dataset)[var].mean().data) for var in model.par}\n",
    "print(mean_ess_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.weight': 0.4712754299075291, '1.bias': 0.13311773795785092}\n"
     ]
    }
   ],
   "source": [
    "mean_mcse_values={var:float(az.mcse(dataset)[var].mean().data) for var in model.par}\n",
    "print(mean_mcse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo,loos,ks=psisloo(log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6273"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ks>0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ks[ks>1]=1\n",
    "sns.histplot(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
