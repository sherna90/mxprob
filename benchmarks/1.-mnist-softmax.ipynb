{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.,1.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "model_ctx = mx.gpu()\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 256 \n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST(train=True).transform_first(transform),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.MNIST(train=False).transform_first(transform),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "(256, 1, 28, 28)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "for X,y in val_data:\n",
    "    print(type(X))\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference for MNIST\n",
    "\n",
    "* [Stochastic Gradient Descent](#chapter1)\n",
    "* [Stochastic Gradient Langevin Dynamics](#chapter2)\n",
    "* [Bayes By Backprop](#chapter3)\n",
    "* [Diagnostics](#chapter4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "hyper={'alpha':10.}\n",
    "in_units=(28,28)\n",
    "out_units=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from hamiltonian.inference.sgd import sgd\n",
    "from hamiltonian.models.softmax import softmax\n",
    "\n",
    "model=softmax(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgd(model,model.par,step_size=0.001,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules re-loaded\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgd)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': Parameter (shape=(10, 784), dtype=float32),\n",
       " '1.bias': Parameter (shape=(10,), dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "train_sgd=False\n",
    "num_epochs=100\n",
    "if train_sgd:\n",
    "    par,loss=inference.fit(epochs=num_epochs,batch_size=batch_size,data_loader=train_data,verbose=True)\n",
    "\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss,color='blue',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGD Softmax MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgd_softmax.pdf', bbox_inches='tight')\n",
    "    model.net.save_parameters('../scripts/results/softmax/softmax_sgd_'+str(num_epochs)+'_epochs.params')\n",
    "else:\n",
    "    model.net.load_parameters('../scripts/results/softmax/softmax_sgd_'+str(num_epochs)+'_epochs.params',ctx=model_ctx)\n",
    "    par=dict()\n",
    "    for name,gluon_par in model.net.collect_params().items():\n",
    "        par.update({name:gluon_par.data()})\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       979\n",
      "           1       0.98      0.97      0.97      1133\n",
      "           2       0.92      0.87      0.90      1030\n",
      "           3       0.88      0.91      0.89      1008\n",
      "           4       0.91      0.93      0.92       980\n",
      "           5       0.84      0.87      0.85       890\n",
      "           6       0.92      0.95      0.93       956\n",
      "           7       0.90      0.92      0.91      1027\n",
      "           8       0.87      0.86      0.86       973\n",
      "           9       0.92      0.85      0.88      1008\n",
      "\n",
      "    accuracy                           0.91      9984\n",
      "   macro avg       0.91      0.91      0.91      9984\n",
      "weighted avg       0.91      0.91      0.91      9984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "total_samples,total_labels,log_like=inference.predict(par,batch_size=batch_size,num_samples=100,data_loader=val_data)\n",
    "y_hat=np.quantile(total_samples,.5,axis=0)\n",
    "print(classification_report(np.int32(total_labels),np.int32(y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Langevin Dynamics <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.inference.sgld import sgld\n",
    "\n",
    "model=softmax(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgld(model,model.par,step_size=0.01,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules re-loaded\n"
     ]
    }
   ],
   "source": [
    "import hamiltonian\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    importlib.reload(hamiltonian.models.softmax)\n",
    "    importlib.reload(hamiltonian.inference.sgld)\n",
    "    print('modules re-loaded')\n",
    "except:\n",
    "    print('no modules loaded yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "train_sgld=False\n",
    "num_epochs=100\n",
    "\n",
    "if train_sgld:\n",
    "    loss,posterior_samples=inference.sample(epochs=num_epochs,batch_size=batch_size,\n",
    "                                data_loader=train_data,\n",
    "                                verbose=True,chain_name='chain_nonhierarchical')\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 360\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss[0],color='blue',lw=3)\n",
    "    plt.plot(loss[1],color='red',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGLD Softmax MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgld_softmax.pdf', bbox_inches='tight')\n",
    "else:\n",
    "    chain1=glob.glob(\"../scripts/results/softmax/chain_nonhierarchical_0_1_sgld*\")\n",
    "    chain2=glob.glob(\"../scripts/results/softmax/chain_nonhierarchical_0_sgld*\")\n",
    "    chain1.sort()\n",
    "    chain2.sort()\n",
    "    posterior_samples=[chain1,chain2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_flat=[item for sublist in posterior_samples for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(posterior_samples_flat,5,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean f-1 : 0.9163494521048552, std f-1 : 0.009028858596384338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score=[]\n",
    "for q in np.arange(.35,.75,.1):\n",
    "    y_hat=np.quantile(total_samples,q,axis=0)\n",
    "    score.append(f1_score(np.int32(total_labels),np.int32(y_hat), average='macro'))\n",
    "print('mean f-1 : {0}, std f-1 : {1}'.format(np.mean(score),2*np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "posterior_samples_multiple_chains=inference.posterior_diagnostics(posterior_samples)\n",
    "samples={var:np.concatenate([posterior_samples_multiple_chains[i][var] \n",
    "                        for i in range(len(posterior_samples_multiple_chains))]) \n",
    "                        for var in model.par}\n",
    "samples={var:np.swapaxes(samples[var],0,1) for var in model.par}\n",
    "r_hat_estimate = lambda samples : tfp.mcmc.diagnostic.potential_scale_reduction(samples, independent_chain_ndims=1,split_chains=True).numpy()\n",
    "rhat = {var:np.median(r_hat_estimate(samples[var])) for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 3.9110374, '1.bias': 1.7014743}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_estimate = lambda samples : tfp.mcmc.diagnostic.effective_sample_size(samples, filter_beyond_positive_pairs=True,cross_chain_dims=1).numpy()\n",
    "ess = {var:np.median(ess_estimate(samples[var])) for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 3.085601, '1.bias': 8.5339775}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.utils.diagnostics import *\n",
    "\n",
    "posterior_samples_multiple_chains=inference.posterior_diagnostics(posterior_samples,serialize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da \n",
    "\n",
    "df=h5py.File('posterior_samples.h5','r')\n",
    "samples={var:da.from_array(df[var]) for var in df.keys()}\n",
    "r_hat_estimate={var:potential_scale_reduction(samples[var]) for var in samples.keys()}\n",
    "ess_estimate={var:effective_sample_size(samples[var]) for var in samples.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = {var:np.median(ess_estimate[var]) for var in model.par}\n",
    "rhat = {var:np.median(r_hat_estimate[var])  for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 1.6918622967224088, '1.bias': 6.755600823881087}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 1.6774608, '1.bias': 1.106829}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "posterior_samples_multiple_chains=inference.posterior_diagnostics(posterior_samples)\n",
    "datasets=[az.convert_to_inference_data(sample) for sample in posterior_samples_multiple_chains]\n",
    "dataset = az.concat(datasets, dim=\"chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r_hat_values={var:float(az.rhat(dataset)[var].median().data) for var in model.par}\n",
    "mean_ess_values={var:float(az.ess(dataset)[var].median().data) for var in model.par}\n",
    "mean_mcse_values={var:float(az.mcse(dataset)[var].median().data) for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.weight': 1.6655441421861479, '1.bias': 1.2943451036823195}\n"
     ]
    }
   ],
   "source": [
    "print(mean_r_hat_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 3.5239685309484114, '1.bias': 5.9626626923120085}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_ess_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mcse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.utils.psis import *\n",
    "\n",
    "loo,loos,ks=psisloo(log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ks=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks[np.isinf(ks)]=max_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "for q in np.arange(.1,.9,.1):\n",
    "    y_hat=np.quantile(total_samples,q,axis=0)\n",
    "    score.append(f1_score(np.int32(total_labels),np.int32(y_hat), sample_weight=1-np.clip(ks,0,1),average='weighted'))\n",
    "print('mean f-1 : {0}, std f-1 : {1}'.format(np.mean(score),2*np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 360\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig=plt.figure(figsize=[5,5])\n",
    "plt.scatter(list(range(len(ks))),ks)\n",
    "plt.plot(list(range(len(ks))), 0.7*(np.ones(len(ks))), linestyle='-',color='red')  # solid\n",
    "plt.xlabel('Data point', size=18)\n",
    "plt.ylabel('Pareto shape k', size=18)\n",
    "plt.title('Non-hierarchical model', size=18)\n",
    "plt.savefig('psis_sgld_softmax.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ks>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks[np.logical_and(ks>0.7,ks<1)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks[np.logical_and(ks>0.5,ks<0.7)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ks<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Softmax <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.models.softmax import hierarchical_softmax\n",
    "from hamiltonian.inference.sgld import sgld\n",
    "\n",
    "model=hierarchical_softmax(hyper,in_units,out_units,ctx=model_ctx)\n",
    "inference=sgld(model,model.par,step_size=0.01,ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgld=False\n",
    "num_epochs=100\n",
    "\n",
    "if train_sgld:\n",
    "    loss,posterior_samples=inference.sample(epochs=num_epochs,batch_size=batch_size,\n",
    "                                data_loader=train_data,\n",
    "                                verbose=True,chain_name='chain_hierarchical')\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 360\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig=plt.figure(figsize=[5,5])\n",
    "    plt.plot(loss[0],color='blue',lw=3)\n",
    "    plt.plot(loss[1],color='red',lw=3)\n",
    "    plt.xlabel('Epoch', size=18)\n",
    "    plt.ylabel('Loss', size=18)\n",
    "    plt.title('SGLD Hierarchical Softmax MNIST', size=18)\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.savefig('sgld_hierarchical_softmax.pdf', bbox_inches='tight')\n",
    "else:\n",
    "    chain1=glob.glob(\"../scripts/results/softmax/chain_hierarchical_0_1_sgld*\")\n",
    "    chain2=glob.glob(\"../scripts/results/softmax/chain_hierarchical_0_sgld*\")\n",
    "    chain1.sort()\n",
    "    chain2.sort()\n",
    "    posterior_samples=[chain1,chain2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_flat=[item for sublist in posterior_samples for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples,total_labels,log_like=inference.predict(posterior_samples_flat,5,data_loader=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean f-1 : 0.91585, std f-1 : 0.006073713855624048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score=[]\n",
    "for q in np.arange(.35,.75,.1):\n",
    "    y_hat=np.quantile(total_samples,q,axis=0)\n",
    "    score.append(f1_score(np.int32(total_labels),np.int32(y_hat), average='micro'))\n",
    "print('mean f-1 : {0}, std f-1 : {1}'.format(np.mean(score),2*np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "posterior_samples_multiple_chains=inference.posterior_diagnostics(posterior_samples)\n",
    "samples={var:np.concatenate([posterior_samples_multiple_chains[i][var] \n",
    "                        for i in range(len(posterior_samples_multiple_chains))]) \n",
    "                        for var in model.par}\n",
    "samples={var:np.swapaxes(samples[var],0,1) for var in model.par}\n",
    "r_hat_estimate = lambda samples : tfp.mcmc.diagnostic.potential_scale_reduction(samples, independent_chain_ndims=1,split_chains=True).numpy()\n",
    "rhat = {var:np.median(r_hat_estimate(samples[var])) for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 3.7939584, '1.bias': 1.1021233}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_estimate = lambda samples : tfp.mcmc.diagnostic.effective_sample_size(samples, filter_beyond_positive_pairs=True,cross_chain_dims=1).numpy()\n",
    "ess = {var:np.median(ess_estimate(samples[var])) for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 3.1676974, '1.bias': 52.368546}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm posterior_samples.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'hamiltonian.utils.diagnostics' from '/home/sergio/code/mxprob/benchmarks/../hamiltonian/utils/diagnostics.py'>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(hamiltonian.utils.diagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamiltonian.utils.diagnostics import *\n",
    "\n",
    "posterior_samples_multiple_chains=inference.posterior_diagnostics(posterior_samples,serialize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da \n",
    "\n",
    "df=h5py.File('posterior_samples.h5','r')\n",
    "samples={var:da.from_array(df[var]) for var in df.keys()}\n",
    "r_hat_estimate={var:potential_scale_reduction(samples[var]) for var in samples.keys()}\n",
    "ess_estimate={var:effective_sample_size(samples[var]) for var in samples.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = {var:np.median(ess_estimate[var]) for var in model.par}\n",
    "rhat = {var:np.median(r_hat_estimate[var])  for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 1.7391584476858615, '1.bias': 34.996306436053935}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.weight': 1.6387938, '1.bias': 1.0146148}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "posterior_samples_multiple_chains=inference.posterior_diagnostics(posterior_samples)\n",
    "datasets=[az.convert_to_inference_data(sample) for sample in posterior_samples_multiple_chains]\n",
    "dataset = az.concat(datasets, dim=\"chain\")\n",
    "mean_r_hat_values={var:float(az.rhat(dataset)[var].mean().data) for var in model.par}\n",
    "mean_ess_values={var:float(az.ess(dataset)[var].mean().data) for var in model.par}\n",
    "mean_mcse_values={var:float(az.mcse(dataset)[var].mean().data) for var in model.par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.weight[0,0]</th>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-1.392</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.174</td>\n",
       "      <td>6.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,1]</th>\n",
       "      <td>-0.538</td>\n",
       "      <td>0.454</td>\n",
       "      <td>-1.575</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.125</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,2]</th>\n",
       "      <td>-0.024</td>\n",
       "      <td>1.951</td>\n",
       "      <td>-2.739</td>\n",
       "      <td>2.450</td>\n",
       "      <td>1.267</td>\n",
       "      <td>1.037</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,3]</th>\n",
       "      <td>-0.790</td>\n",
       "      <td>0.887</td>\n",
       "      <td>-2.373</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.343</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.weight[0,4]</th>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.173</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[5]</th>\n",
       "      <td>0.733</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.352</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.026</td>\n",
       "      <td>29.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[6]</th>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.021</td>\n",
       "      <td>47.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[7]</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>87.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[8]</th>\n",
       "      <td>-0.577</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.022</td>\n",
       "      <td>39.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.bias[9]</th>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.016</td>\n",
       "      <td>59.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7850 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "1.weight[0,0] -0.435  0.559  -1.392    0.534      0.233    0.174       6.0   \n",
       "1.weight[0,1] -0.538  0.454  -1.575    0.331      0.169    0.125       7.0   \n",
       "1.weight[0,2] -0.024  1.951  -2.739    2.450      1.267    1.037       3.0   \n",
       "1.weight[0,3] -0.790  0.887  -2.373    0.713      0.447    0.343       4.0   \n",
       "1.weight[0,4] -0.142  0.515  -0.970    0.821      0.230    0.173       5.0   \n",
       "...              ...    ...     ...      ...        ...      ...       ...   \n",
       "1.bias[5]      0.733  0.204   0.352    1.079      0.037    0.026      29.0   \n",
       "1.bias[6]     -0.097  0.168  -0.413    0.209      0.025    0.021      47.0   \n",
       "1.bias[7]      0.222  0.191  -0.121    0.554      0.020    0.014      87.0   \n",
       "1.bias[8]     -0.577  0.197  -0.903   -0.223      0.031    0.022      39.0   \n",
       "1.bias[9]     -0.086  0.180  -0.383    0.243      0.023    0.016      59.0   \n",
       "\n",
       "               ess_tail  r_hat  \n",
       "1.weight[0,0]      54.0   1.28  \n",
       "1.weight[0,1]      12.0   1.27  \n",
       "1.weight[0,2]      12.0   1.89  \n",
       "1.weight[0,3]      12.0   1.51  \n",
       "1.weight[0,4]      21.0   1.35  \n",
       "...                 ...    ...  \n",
       "1.bias[5]         106.0   1.05  \n",
       "1.bias[6]         104.0   1.04  \n",
       "1.bias[7]          75.0   1.06  \n",
       "1.bias[8]         131.0   1.08  \n",
       "1.bias[9]          37.0   1.05  \n",
       "\n",
       "[7850 rows x 9 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.weight': 1.6711408591815198, '1.bias': 1.0529862404615036}\n"
     ]
    }
   ],
   "source": [
    "print(mean_r_hat_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.weight': 4.9645475053530435, '1.bias': 57.65651747515627}\n"
     ]
    }
   ],
   "source": [
    "print(mean_ess_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_mcse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo,loos,ks=psisloo(log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "for q in np.arange(.1,.9,.1):\n",
    "    y_hat=np.quantile(total_samples,q,axis=0)\n",
    "    score.append(f1_score(np.int32(total_labels[ks>0.7]),np.int32(y_hat[ks>0.7]), average='macro'))\n",
    "print('mean f-1 : {0}, std f-1 : {1}'.format(np.mean(score),2*np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "for q in np.arange(.1,.9,.1):\n",
    "    y_hat=np.quantile(total_samples,q,axis=0)\n",
    "    score.append(f1_score(np.int32(total_labels[ks<0.7]),np.int32(y_hat[ks<0.7]), average='macro'))\n",
    "print('mean f-1 : {0}, std f-1 : {1}'.format(np.mean(score),2*np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "for q in np.arange(.1,.9,.1):\n",
    "    y_hat=np.quantile(total_samples,q,axis=0)\n",
    "    score.append(f1_score(np.int32(total_labels),np.int32(y_hat), sample_weight=1-np.clip(ks,0,1),average='weighted'))\n",
    "print('mean f-1 : {0}, std f-1 : {1}'.format(np.mean(score),2*np.std(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 360\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig=plt.figure(figsize=[5,5])\n",
    "plt.scatter(list(range(len(ks))),ks)\n",
    "plt.plot(list(range(len(ks))), 0.7*(np.ones(len(ks))), linestyle='-',color='red')  # solid\n",
    "plt.xlabel('Data point', size=18)\n",
    "plt.ylabel('Pareto shape k', size=18)\n",
    "plt.title('Hierarchical model', size=18)\n",
    "plt.savefig('psis_sgld_hierarchical_softmax.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ks>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks[np.logical_and(ks>0.7,ks<1)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks[np.logical_and(ks>0.5,ks<0.7)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ks<0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
